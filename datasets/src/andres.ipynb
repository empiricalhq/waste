{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pandas as pd\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Pandas:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f12e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.parse import unquote\n",
    "from data_downloader import download\n",
    "\n",
    "# Notebook paths\n",
    "NB_DIR   = Path.cwd()\n",
    "BASE_DIR = NB_DIR.parent                   # .../datasets\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "ROOT_DIR = BASE_DIR                        # .../datasets (raÃ­z)\n",
    "\n",
    "URLS = {\n",
    "    \"org\": \"https://datosabiertos.gob.pe/dataset/valorizaci%C3%B3n-de-residuos-s%C3%B3lidos-nivel-distrital-ministerio-del-ambiente-minam\",\n",
    "    \"inorg\": \"https://datosabiertos.gob.pe/dataset/valorizaci%C3%B3n-de-residuos-s%C3%B3lidos-nivel-distrital-ministerio-del-ambiente-minam\"\n",
    "}\n",
    "\n",
    "downloaded_files = download(list(URLS.values()), DATA_DIR)\n",
    "\n",
    "def find_file(name_part: str):\n",
    "    for f in downloaded_files:\n",
    "        decoded_name = unquote(f.name)\n",
    "        if name_part in decoded_name:\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "PATH_ORG   = find_file(\"Org\")\n",
    "PATH_INORG = find_file(\"Inorg\")\n",
    "\n",
    "assert PATH_ORG   is not None, \"No se encontrÃ³ el CSV de Residuos OrgÃ¡nicos\"\n",
    "assert PATH_INORG is not None, \"No se encontrÃ³ el CSV de Residuos InorgÃ¡nicos\"\n",
    "\n",
    "print(\"ORG:\", PATH_ORG)\n",
    "print(\"INORG:\", PATH_INORG)\n",
    "\n",
    "# Carga robusta (CSV del MINAM suelen venir con ; y lÃ­neas raras)\n",
    "df_org   = pd.read_csv(PATH_ORG,   sep=\";\", on_bad_lines=\"skip\")\n",
    "df_inorg = pd.read_csv(PATH_INORG, sep=\";\", on_bad_lines=\"skip\")\n",
    "\n",
    "print(\"ðŸ”Ž OrgÃ¡nicos:\",   df_org.shape,   \"filas\")\n",
    "print(\"ðŸ”Ž InorgÃ¡nicos:\", df_inorg.shape, \"filas\")\n",
    "display(df_org.head(3)); display(df_inorg.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pandas as pd\n",
    "\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # quita tildes, pasa a mayÃºscula y deja solo A-Z0-9 con _ como separador\n",
    "    df.columns = (df.columns\n",
    "                  .str.normalize('NFKD').str.encode('ascii','ignore').str.decode('ascii')\n",
    "                  .str.upper().str.strip()\n",
    "                  .str.replace(r'[^A-Z0-9]+', '_', regex=True)\n",
    "                  .str.strip('_'))\n",
    "    return df\n",
    "\n",
    "df_org  = normalize_cols(df_org)\n",
    "df_inorg= normalize_cols(df_inorg)\n",
    "\n",
    "print(\"ORG cols:\", df_org.columns.tolist())\n",
    "print(\"INORG cols:\", df_inorg.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e11fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_col(df, must_have=(), nice_to_have=()):\n",
    "    cols = list(df.columns)\n",
    "    # 1) match fuerte: todas las must_have\n",
    "    for c in cols:\n",
    "        if all(m in c for m in must_have):\n",
    "            return c\n",
    "    # 2) match suave: mÃ¡ximo nÂº de nice_to_have\n",
    "    if nice_to_have:\n",
    "        scored = sorted(cols, key=lambda c: sum(n in c for n in nice_to_have), reverse=True)\n",
    "        return scored[0]\n",
    "    return None\n",
    "\n",
    "# Para ambos datasets (tienen mismas estructuras pero distintas \"familias\")\n",
    "col_dist   = find_col(df_org, must_have=(\"DISTRITO\",))\n",
    "col_ubigeo = find_col(df_org, must_have=(\"UBIGEO\",))\n",
    "col_dep    = find_col(df_org, must_have=(\"DEPARTAMENTO\",))\n",
    "col_prov   = find_col(df_org, must_have=(\"PROVINCIA\",))\n",
    "\n",
    "# MÃ©tricas clave\n",
    "col_val_org    = find_col(df_org,    must_have=(\"VALORIZ\",), nice_to_have=(\"T\",\"TON\",\"TONELADA\"))\n",
    "col_val_inorg  = find_col(df_inorg,  must_have=(\"VALORIZ\",), nice_to_have=(\"T\",\"TON\",\"TONELADA\"))\n",
    "\n",
    "# Generado total (puede venir en uno de los dos)\n",
    "col_gen_org    = find_col(df_org,    must_have=(\"RESIDUOS\",\"GENER\"))\n",
    "col_gen_inorg  = find_col(df_inorg,  must_have=(\"RESIDUOS\",\"GENER\"))\n",
    "col_gen = col_gen_org or col_gen_inorg\n",
    "\n",
    "# PoblaciÃ³n\n",
    "col_pob_org    = find_col(df_org,    must_have=(\"POB\",\"TOTAL\"))\n",
    "col_pob_inorg  = find_col(df_inorg,  must_have=(\"POB\",\"TOTAL\"))\n",
    "col_pob = col_pob_org or col_pob_inorg\n",
    "\n",
    "print(\"ðŸ’¡ detectado ->\", {\n",
    "    \"UBIGEO\": col_ubigeo, \"DISTRITO\": col_dist, \"DEP\": col_dep, \"PROV\": col_prov,\n",
    "    \"VAL_ORG\": col_val_org, \"VAL_INORG\": col_val_inorg, \"GEN_T\": col_gen, \"POB_TOTAL\": col_pob\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9707cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(s):\n",
    "    # deja dÃ­gitos, signo y punto decimal\n",
    "    return pd.to_numeric(s.astype(str).str.replace(r'[^0-9\\.\\-]', '', regex=True),\n",
    "                         errors='coerce')\n",
    "\n",
    "# Copias de trabajo\n",
    "org  = df_org.copy()\n",
    "inorg= df_inorg.copy()\n",
    "\n",
    "# Convierte\n",
    "for (df, col_val) in [(org, col_val_org), (inorg, col_val_inorg)]:\n",
    "    if col_val and col_val in df.columns:\n",
    "        df[col_val] = to_float(df[col_val])\n",
    "\n",
    "# Generado y poblaciÃ³n (si existen)\n",
    "if col_gen and col_gen in org.columns:   org[col_gen]   = to_float(org[col_gen])\n",
    "if col_gen and col_gen in inorg.columns: inorg[col_gen] = to_float(inorg[col_gen])\n",
    "if col_pob and col_pob in org.columns:   org[col_pob]   = to_float(org[col_pob])\n",
    "if col_pob and col_pob in inorg.columns: inorg[col_pob] = to_float(inorg[col_pob])\n",
    "\n",
    "# % valorizado (si hay generado en el mismo df)\n",
    "if col_gen in org.columns and col_val_org:\n",
    "    org[\"PORC_VALORIZADO\"] = (org[col_val_org] / org[col_gen]) * 100\n",
    "if col_gen in inorg.columns and col_val_inorg:\n",
    "    inorg[\"PORC_VALORIZADO\"] = (inorg[col_val_inorg] / inorg[col_gen]) * 100\n",
    "\n",
    "# Renombra a canÃ³nicos para graficar simple\n",
    "org = org.rename(columns={\n",
    "    col_dist: \"DISTRITO\", col_dep: \"DEPARTAMENTO\", col_prov: \"PROVINCIA\",\n",
    "    col_val_org: \"ORG_TON\", col_gen or \"\": \"GENERADO_TON\", col_ubigeo: \"UBIGEO\",\n",
    "    col_pob or \"\": \"POB_TOTAL\"\n",
    "})\n",
    "inorg = inorg.rename(columns={\n",
    "    col_dist: \"DISTRITO\", col_dep: \"DEPARTAMENTO\", col_prov: \"PROVINCIA\",\n",
    "    col_val_inorg: \"INORG_TON\", col_gen or \"\": \"GENERADO_TON\", col_ubigeo: \"UBIGEO\",\n",
    "    col_pob or \"\": \"POB_TOTAL\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f9642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Suma por distrito (puede haber varias filas por distrito/aÃ±o)\n",
    "org_g   = org.groupby([\"DEPARTAMENTO\",\"PROVINCIA\",\"DISTRITO\"], as_index=False)[\"ORG_TON\"].sum()\n",
    "inorg_g = inorg.groupby([\"DEPARTAMENTO\",\"PROVINCIA\",\"DISTRITO\"], as_index=False)[\"INORG_TON\"].sum()\n",
    "\n",
    "df_total = (org_g.merge(inorg_g, on=[\"DEPARTAMENTO\",\"PROVINCIA\",\"DISTRITO\"], how=\"outer\")\n",
    "                 .fillna(0.0))\n",
    "df_total[\"TOTAL\"] = df_total[\"ORG_TON\"] + df_total[\"INORG_TON\"]\n",
    "\n",
    "top10 = df_total.sort_values(\"TOTAL\", ascending=False).head(10)\n",
    "top10[\"ETIQUETA\"] = top10[\"DISTRITO\"] + \" (\" + top10[\"PROVINCIA\"] + \")\"\n",
    "\n",
    "fig = px.bar(\n",
    "    top10, x=\"ETIQUETA\", y=[\"ORG_TON\",\"INORG_TON\"],\n",
    "    barmode=\"stack\", text_auto=True,\n",
    "    title=\"Top 10 distritos por valorizaciÃ³n total (t)\"\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"\", yaxis_title=\"Toneladas valorizadas\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ecf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPIs\n",
    "total_org  = org[\"ORG_TON\"].sum(skipna=True)\n",
    "total_inorg= inorg[\"INORG_TON\"].sum(skipna=True)\n",
    "print(f\"â™»ï¸ Total valorizado orgÃ¡nico (t): {total_org:,.0f}\")\n",
    "print(f\"â™»ï¸ Total valorizado inorgÃ¡nico (t): {total_inorg:,.0f}\")\n",
    "\n",
    "# % distritos con â‰¥20% (si pudiste calcular PORC_VALORIZADO)\n",
    "def pct_ge_20(df):\n",
    "    if \"PORC_VALORIZADO\" in df.columns:\n",
    "        valid = df[\"PORC_VALORIZADO\"].dropna()\n",
    "        return (valid >= 20).mean()*100 if len(valid) else float(\"nan\")\n",
    "    return float(\"nan\")\n",
    "\n",
    "print(f\"ðŸ Distritos â‰¥20% (org): {pct_ge_20(org):.1f}%\")\n",
    "print(f\"ðŸ Distritos â‰¥20% (inorg): {pct_ge_20(inorg):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9883a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [markdown]\n",
    "# # ðŸŽ¯ Setup y Carga de Datos Optimizada\n",
    "\n",
    "#%%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e812cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfiguraciÃ³n de visualizaciÃ³n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db073c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos con encoding correcto para caracteres especiales\n",
    "def load_waste_data():\n",
    "    \"\"\"Carga robusta de datos del MINAM con manejo de errores\"\"\"\n",
    "    \n",
    "    # Buscar archivos en diferentes ubicaciones posibles\n",
    "    possible_paths = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd() / \"data\",\n",
    "        Path.cwd().parent / \"data\",\n",
    "    ]\n",
    "    \n",
    "    org_file = None\n",
    "    inorg_file = None\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if path.exists():\n",
    "            for f in path.iterdir():\n",
    "                if not org_file and \"Org\" in f.name:\n",
    "                    org_file = f\n",
    "                if not inorg_file and \"Inorg\" in f.name:\n",
    "                    inorg_file = f\n",
    "                if org_file and inorg_file:\n",
    "                    break\n",
    "        if org_file and inorg_file:\n",
    "            break\n",
    "    \n",
    "    # Cargar con separador correcto (;) y encoding latino\n",
    "    df_org = pd.read_csv(org_file, sep=';', encoding='latin-1', on_bad_lines='skip')\n",
    "    df_inorg = pd.read_csv(inorg_file, sep=';', encoding='latin-1', on_bad_lines='skip')\n",
    "    \n",
    "    return df_org, df_inorg\n",
    "\n",
    "df_org_raw, df_inorg_raw = load_waste_data()\n",
    "print(f\"âœ… Datos cargados: {len(df_org_raw)} registros orgÃ¡nicos, {len(df_inorg_raw)} registros inorgÃ¡nicos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [markdown]\n",
    "# # ðŸ”§ Limpieza y NormalizaciÃ³n de Datos\n",
    "\n",
    "#%%\n",
    "def clean_numeric_columns(df):\n",
    "    \"\"\"Convierte columnas numÃ©ricas con formato peruano (comas) a float\"\"\"\n",
    "    numeric_cols = ['POB_TOTAL', 'POB_URBANA', 'POB_RURAL', 'QRESIDUOS_MUN']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            # Remover comas y convertir a float\n",
    "            df[col] = df[col].astype(str).str.replace(',', '').str.replace(' ', '')\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def normalize_column_names(df):\n",
    "    \"\"\"Normaliza nombres de columnas quitando espacios\"\"\"\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "# Procesar ambos datasets\n",
    "df_org = df_org_raw.copy()\n",
    "df_inorg = df_inorg_raw.copy()\n",
    "\n",
    "# Normalizar columnas\n",
    "df_org = normalize_column_names(df_org)\n",
    "df_inorg = normalize_column_names(df_inorg)\n",
    "\n",
    "# Identificar columnas de valorizaciÃ³n (tienen espacios problemÃ¡ticos)\n",
    "val_org_col = [col for col in df_org.columns if 'VAL_ORGAN' in col][0]\n",
    "val_inorg_col = [col for col in df_inorg.columns if 'VAL_INORGAN' in col][0]\n",
    "\n",
    "print(f\"ðŸ“Š Columna orgÃ¡nicos: '{val_org_col}'\")\n",
    "print(f\"ðŸ“Š Columna inorgÃ¡nicos: '{val_inorg_col}'\")\n",
    "\n",
    "# Limpiar datos numÃ©ricos\n",
    "df_org = clean_numeric_columns(df_org)\n",
    "df_inorg = clean_numeric_columns(df_inorg)\n",
    "\n",
    "# Limpiar columnas de valorizaciÃ³n\n",
    "df_org[val_org_col] = pd.to_numeric(df_org[val_org_col], errors='coerce')\n",
    "df_inorg[val_inorg_col] = pd.to_numeric(df_inorg[val_inorg_col], errors='coerce')\n",
    "\n",
    "# Renombrar para facilitar el trabajo\n",
    "df_org = df_org.rename(columns={val_org_col: 'VAL_ORGAN'})\n",
    "df_inorg = df_inorg.rename(columns={val_inorg_col: 'VAL_INORGAN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d347a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [markdown]\n",
    "# # ðŸ“ˆ INSIGHT 1: AnÃ¡lisis Temporal - EvoluciÃ³n de la ValorizaciÃ³n\n",
    "\n",
    "#%%\n",
    "# AnÃ¡lisis por aÃ±o\n",
    "yearly_stats = []\n",
    "\n",
    "for year in df_org['PERIODO'].unique():\n",
    "    org_year = df_org[df_org['PERIODO'] == year]\n",
    "    inorg_year = df_inorg[df_inorg['PERIODO'] == year]\n",
    "    \n",
    "    stats = {\n",
    "        'AÃ±o': year,\n",
    "        'Residuos_Generados_Ton': org_year['QRESIDUOS_MUN'].sum(),\n",
    "        'ValorizaciÃ³n_OrgÃ¡nica_Ton': org_year['VAL_ORGAN'].sum(),\n",
    "        'ValorizaciÃ³n_InorgÃ¡nica_Ton': inorg_year['VAL_INORGAN'].sum(),\n",
    "        'ValorizaciÃ³n_Total_Ton': org_year['VAL_ORGAN'].sum() + inorg_year['VAL_INORGAN'].sum(),\n",
    "        'Tasa_ValorizaciÃ³n_%': ((org_year['VAL_ORGAN'].sum() + inorg_year['VAL_INORGAN'].sum()) / \n",
    "                                org_year['QRESIDUOS_MUN'].sum() * 100) if org_year['QRESIDUOS_MUN'].sum() > 0 else 0\n",
    "    }\n",
    "    yearly_stats.append(stats)\n",
    "\n",
    "df_yearly = pd.DataFrame(yearly_stats).sort_values('AÃ±o')\n",
    "\n",
    "# VisualizaciÃ³n de tendencias\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# GrÃ¡fico 1: EvoluciÃ³n de residuos generados vs valorizados\n",
    "ax1 = axes[0, 0]\n",
    "x = df_yearly['AÃ±o']\n",
    "ax1.bar(x - 0.2, df_yearly['Residuos_Generados_Ton']/1000000, 0.4, label='Generados', alpha=0.7, color='#e74c3c')\n",
    "ax1.bar(x + 0.2, df_yearly['ValorizaciÃ³n_Total_Ton']/1000, 0.4, label='Valorizados (x1000)', alpha=0.7, color='#27ae60')\n",
    "ax1.set_xlabel('AÃ±o')\n",
    "ax1.set_ylabel('Toneladas')\n",
    "ax1.set_title('ðŸ“Š Residuos Generados (Millones ton) vs Valorizados (Miles ton)', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# GrÃ¡fico 2: Tasa de valorizaciÃ³n\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(df_yearly['AÃ±o'], df_yearly['Tasa_ValorizaciÃ³n_%'], marker='o', linewidth=3, markersize=10, color='#3498db')\n",
    "ax2.fill_between(df_yearly['AÃ±o'], 0, df_yearly['Tasa_ValorizaciÃ³n_%'], alpha=0.3)\n",
    "ax2.set_xlabel('AÃ±o')\n",
    "ax2.set_ylabel('Tasa de ValorizaciÃ³n (%)')\n",
    "ax2.set_title('ðŸ“ˆ EvoluciÃ³n de la Tasa de ValorizaciÃ³n', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# AÃ±adir valores en los puntos\n",
    "for i, row in df_yearly.iterrows():\n",
    "    ax2.annotate(f'{row[\"Tasa_ValorizaciÃ³n_%\"]:.2f}%', \n",
    "                xy=(row['AÃ±o'], row['Tasa_ValorizaciÃ³n_%']),\n",
    "                xytext=(0, 10), textcoords='offset points',\n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# GrÃ¡fico 3: ComposiciÃ³n orgÃ¡nica vs inorgÃ¡nica\n",
    "ax3 = axes[1, 0]\n",
    "width = 0.6\n",
    "ax3.bar(df_yearly['AÃ±o'], df_yearly['ValorizaciÃ³n_OrgÃ¡nica_Ton']/1000, width, \n",
    "        label='OrgÃ¡nica', color='#27ae60', alpha=0.8)\n",
    "ax3.bar(df_yearly['AÃ±o'], df_yearly['ValorizaciÃ³n_InorgÃ¡nica_Ton']/1000, width,\n",
    "        bottom=df_yearly['ValorizaciÃ³n_OrgÃ¡nica_Ton']/1000,\n",
    "        label='InorgÃ¡nica', color='#3498db', alpha=0.8)\n",
    "ax3.set_xlabel('AÃ±o')\n",
    "ax3.set_ylabel('Miles de Toneladas')\n",
    "ax3.set_title('ðŸŒ± ComposiciÃ³n de la ValorizaciÃ³n (OrgÃ¡nica vs InorgÃ¡nica)', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# GrÃ¡fico 4: Crecimiento aÃ±o a aÃ±o\n",
    "ax4 = axes[1, 1]\n",
    "df_yearly['Crecimiento_%'] = df_yearly['ValorizaciÃ³n_Total_Ton'].pct_change() * 100\n",
    "colors = ['#27ae60' if x > 0 else '#e74c3c' for x in df_yearly['Crecimiento_%'].fillna(0)]\n",
    "bars = ax4.bar(df_yearly['AÃ±o'][1:], df_yearly['Crecimiento_%'][1:], color=colors, alpha=0.7)\n",
    "ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax4.set_xlabel('AÃ±o')\n",
    "ax4.set_ylabel('Crecimiento (%)')\n",
    "ax4.set_title('ðŸ“Š Tasa de Crecimiento Anual de la ValorizaciÃ³n', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# AÃ±adir valores en las barras\n",
    "for bar, val in zip(bars, df_yearly['Crecimiento_%'][1:]):\n",
    "    if not pd.isna(val):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "                f'{val:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('ðŸš€ ANÃLISIS TEMPORAL DE VALORIZACIÃ“N DE RESIDUOS SÃ“LIDOS - PERÃš', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar tabla resumen\n",
    "print(\"\\nðŸ“Š RESUMEN ESTADÃSTICO POR AÃ‘O:\")\n",
    "print(\"=\"*80)\n",
    "display(df_yearly.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f727e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [markdown]\n",
    "# # ðŸ† INSIGHT 2: Top Performers - Distritos Campeones\n",
    "\n",
    "#%%\n",
    "# AnÃ¡lisis para 2023 (aÃ±o mÃ¡s reciente)\n",
    "df_org_2023 = df_org[df_org['PERIODO'] == 2023].copy()\n",
    "df_inorg_2023 = df_inorg[df_inorg['PERIODO'] == 2023].copy()\n",
    "\n",
    "# Combinar datos\n",
    "df_combined = df_org_2023.merge(\n",
    "    df_inorg_2023[['UBIGEO', 'VAL_INORGAN']], \n",
    "    on='UBIGEO', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_combined['VAL_TOTAL'] = df_combined['VAL_ORGAN'].fillna(0) + df_combined['VAL_INORGAN'].fillna(0)\n",
    "df_combined['TASA_VAL'] = (df_combined['VAL_TOTAL'] / df_combined['QRESIDUOS_MUN'] * 100).round(2)\n",
    "df_combined['DISTRITO_FULL'] = df_combined['DISTRITO'] + ' (' + df_combined['PROVINCIA'] + ')'\n",
    "\n",
    "# Top 15 por valorizaciÃ³n absoluta\n",
    "top15_absoluto = df_combined.nlargest(15, 'VAL_TOTAL')\n",
    "\n",
    "# VisualizaciÃ³n\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# GrÃ¡fico de barras horizontales - Top distritos\n",
    "y_pos = np.arange(len(top15_absoluto))\n",
    "ax1.barh(y_pos, top15_absoluto['VAL_ORGAN'].values, label='OrgÃ¡nica', color='#27ae60', alpha=0.8)\n",
    "ax1.barh(y_pos, top15_absoluto['VAL_INORGAN'].values, left=top15_absoluto['VAL_ORGAN'].values,\n",
    "         label='InorgÃ¡nica', color='#3498db', alpha=0.8)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(top15_absoluto['DISTRITO_FULL'].values, fontsize=10)\n",
    "ax1.set_xlabel('Toneladas Valorizadas')\n",
    "ax1.set_title('ðŸ† TOP 15 DISTRITOS - VALORIZACIÃ“N ABSOLUTA (2023)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# GrÃ¡fico de dispersiÃ³n - RelaciÃ³n entre generaciÃ³n y valorizaciÃ³n\n",
    "ax2.scatter(df_combined['QRESIDUOS_MUN'], df_combined['VAL_TOTAL'], \n",
    "           s=df_combined['POB_TOTAL']/500, alpha=0.6, c=df_combined['TASA_VAL'],\n",
    "           cmap='RdYlGn', edgecolors='black', linewidth=0.5)\n",
    "ax2.set_xlabel('Residuos Generados (ton)')\n",
    "ax2.set_ylabel('Residuos Valorizados (ton)')\n",
    "ax2.set_title('ðŸŽ¯ EFICIENCIA DE VALORIZACIÃ“N POR DISTRITO (2023)', fontweight='bold')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(ax2.collections[0], ax=ax2, label='Tasa ValorizaciÃ³n (%)')\n",
    "\n",
    "# AÃ±adir lÃ­neas de referencia para diferentes tasas\n",
    "for rate in [1, 5, 10, 20]:\n",
    "    x_line = np.logspace(1, 5, 100)\n",
    "    y_line = x_line * rate / 100\n",
    "    ax2.plot(x_line, y_line, '--', alpha=0.3, label=f'{rate}%')\n",
    "\n",
    "ax2.legend(loc='upper left', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87442b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [markdown]\n",
    "# # ðŸŒŸ INSIGHT 3: AnÃ¡lisis de Eficiencia - Los MÃ¡s Sostenibles\n",
    "\n",
    "#%%\n",
    "# Filtrar distritos con generaciÃ³n significativa (>100 ton/aÃ±o)\n",
    "df_eficientes = df_combined[df_combined['QRESIDUOS_MUN'] > 100].copy()\n",
    "df_eficientes = df_eficientes.dropna(subset=['TASA_VAL'])\n",
    "\n",
    "# Top 20 mÃ¡s eficientes\n",
    "top20_eficiencia = df_eficientes.nlargest(20, 'TASA_VAL')\n",
    "\n",
    "# Crear categorÃ­as de eficiencia\n",
    "def categorize_efficiency(tasa):\n",
    "    if tasa >= 20: return 'Excelente (â‰¥20%)'\n",
    "    elif tasa >= 10: return 'Bueno (10-20%)'\n",
    "    elif tasa >= 5: return 'Regular (5-10%)'\n",
    "    elif tasa >= 1: return 'Bajo (1-5%)'\n",
    "    else: return 'Muy Bajo (<1%)'\n",
    "\n",
    "df_eficientes['CATEGORIA'] = df_eficientes['TASA_VAL'].apply(categorize_efficiency)\n",
    "\n",
    "# VisualizaciÃ³n\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# GrÃ¡fico 1: Top 20 mÃ¡s eficientes\n",
    "ax1 = axes[0, 0]\n",
    "colors_eff = ['#27ae60' if x >= 20 else '#3498db' if x >= 10 else '#f39c12' if x >= 5 else '#e74c3c' \n",
    "              for x in top20_eficiencia['TASA_VAL']]\n",
    "bars = ax1.bar(range(len(top20_eficiencia)), top20_eficiencia['TASA_VAL'], color=colors_eff, alpha=0.8)\n",
    "ax1.set_xticks(range(len(top20_eficiencia)))\n",
    "ax1.set_xticklabels(top20_eficiencia['DISTRITO'].values, rotation=45, ha='right', fontsize=9)\n",
    "ax1.set_ylabel('Tasa de ValorizaciÃ³n (%)')\n",
    "ax1.set_title('ðŸŒŸ TOP 20 DISTRITOS MÃS EFICIENTES (2023)', fontweight='bold')\n",
    "ax1.axhline(y=20, color='green', linestyle='--', alpha=0.5, label='Meta 20%')\n",
    "ax1.axhline(y=10, color='orange', linestyle='--', alpha=0.5, label='Meta 10%')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# GrÃ¡fico 2: DistribuciÃ³n de eficiencia\n",
    "ax2 = axes[0, 1]\n",
    "categoria_counts = df_eficientes['CATEGORIA'].value_counts()\n",
    "colors_cat = ['#27ae60', '#3498db', '#f39c12', '#e74c3c', '#c0392b']\n",
    "wedges, texts, autotexts = ax2.pie(categoria_counts.values, labels=categoria_counts.index, \n",
    "                                    colors=colors_cat, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('ðŸ“Š DISTRIBUCIÃ“N DE DISTRITOS POR EFICIENCIA', fontweight='bold')\n",
    "\n",
    "# GrÃ¡fico 3: AnÃ¡lisis por departamento\n",
    "ax3 = axes[1, 0]\n",
    "dept_stats = df_combined.groupby('DEPARTAMENTO').agg({\n",
    "    'VAL_TOTAL': 'sum',\n",
    "    'QRESIDUOS_MUN': 'sum'\n",
    "}).reset_index()\n",
    "dept_stats['TASA_DEPT'] = (dept_stats['VAL_TOTAL'] / dept_stats['QRESIDUOS_MUN'] * 100).round(2)\n",
    "dept_stats = dept_stats.nlargest(15, 'TASA_DEPT')\n",
    "\n",
    "bars = ax3.bar(range(len(dept_stats)), dept_stats['TASA_DEPT'], \n",
    "               color=plt.cm.RdYlGn(dept_stats['TASA_DEPT']/dept_stats['TASA_DEPT'].max()), \n",
    "               alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "ax3.set_xticks(range(len(dept_stats)))\n",
    "ax3.set_xticklabels(dept_stats['DEPARTAMENTO'].values, rotation=45, ha='right', fontsize=9)\n",
    "ax3.set_ylabel('Tasa de ValorizaciÃ³n (%)')\n",
    "ax3.set_title('ðŸ—ºï¸ EFICIENCIA POR DEPARTAMENTO (2023)', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# AÃ±adir valores en las barras\n",
    "for bar, val in zip(bars, dept_stats['TASA_DEPT']):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "            f'{val:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# GrÃ¡fico 4: CorrelaciÃ³n poblaciÃ³n vs eficiencia\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(df_eficientes['POB_TOTAL'], df_eficientes['TASA_VAL'],\n",
    "                     s=df_eficientes['QRESIDUOS_MUN']/10, alpha=0.6,\n",
    "                     c=df_eficientes['TASA_VAL'], cmap='RdYlGn',\n",
    "                     edgecolors='black', linewidth=0.5)\n",
    "ax4.set_xlabel('PoblaciÃ³n Total')\n",
    "ax4.set_ylabel('Tasa de ValorizaciÃ³n (%)')\n",
    "ax4.set_title('ðŸ™ï¸ RELACIÃ“N POBLACIÃ“N vs EFICIENCIA', fontweight='bold')\n",
    "ax4.set_xscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax4, label='Tasa (%)')\n",
    "\n",
    "plt.suptitle('ðŸŽ¯ ANÃLISIS DE EFICIENCIA EN VALORIZACIÃ“N DE RESIDUOS', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58029c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [markdown]\n",
    "# # ðŸ’¡ INSIGHT 4: Oportunidades de Mejora - Potencial Sin Explotar\n",
    "\n",
    "#%%\n",
    "# Identificar distritos con alto potencial de mejora\n",
    "df_potencial = df_combined.copy()\n",
    "df_potencial['POTENCIAL_TON'] = df_potencial['QRESIDUOS_MUN'] * 0.2 - df_potencial['VAL_TOTAL']  # Si llegaran al 20%\n",
    "df_potencial = df_potencial[df_potencial['POTENCIAL_TON'] > 0]\n",
    "\n",
    "# Top 20 con mayor potencial absoluto\n",
    "top20_potencial = df_potencial.nlargest(20, 'POTENCIAL_TON')\n",
    "\n",
    "# AnÃ¡lisis de brechas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# GrÃ¡fico 1: Distritos con mayor potencial\n",
    "ax1 = axes[0, 0]\n",
    "y_pos = np.arange(len(top20_potencial))\n",
    "bars1 = ax1.barh(y_pos, top20_potencial['VAL_TOTAL'].values, label='ValorizaciÃ³n Actual', \n",
    "                 color='#3498db', alpha=0.8)\n",
    "bars2 = ax1.barh(y_pos, top20_potencial['POTENCIAL_TON'].values, \n",
    "                 left=top20_potencial['VAL_TOTAL'].values,\n",
    "                 label='Potencial Adicional (hasta 20%)', color='#95a5a6', alpha=0.6)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(top20_potencial['DISTRITO_FULL'].values, fontsize=9)\n",
    "ax1.set_xlabel('Toneladas')\n",
    "ax1.set_title('ðŸ’¡ TOP 20 DISTRITOS CON MAYOR POTENCIAL DE MEJORA', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# GrÃ¡fico 2: Matriz de priorizaciÃ³n\n",
    "ax2 = axes[0, 1]\n",
    "# Cuadrantes: Alto potencial + Alta poblaciÃ³n = Prioridad 1\n",
    "ax2.scatter(df_potencial['POB_TOTAL'], df_potencial['POTENCIAL_TON'],\n",
    "           s=100, alpha=0.6, c=df_potencial['TASA_VAL'], cmap='RdYlGn_r',\n",
    "           edgecolors='black', linewidth=0.5)\n",
    "ax2.axvline(x=df_potencial['POB_TOTAL'].median(), color='red', linestyle='--', alpha=0.5)\n",
    "ax2.axhline(y=df_potencial['POTENCIAL_TON'].median(), color='red', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('PoblaciÃ³n')\n",
    "ax2.set_ylabel('Potencial de Mejora (ton)')\n",
    "ax2.set_title('ðŸŽ¯ MATRIZ DE PRIORIZACIÃ“N DE INTERVENCIONES', fontweight='bold')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# AÃ±adir etiquetas de cuadrantes\n",
    "ax2.text(0.95, 0.95, 'PRIORIDAD 1\\n(Alto impacto)', transform=ax2.transAxes,\n",
    "        ha='right', va='top', fontweight='bold', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='#e74c3c', alpha=0.5))\n",
    "ax2.text(0.05, 0.95, 'PRIORIDAD 2', transform=ax2.transAxes,\n",
    "        ha='left', va='top', fontweight='bold', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='#f39c12', alpha=0.5))\n",
    "ax2.text(0.95, 0.05, 'PRIORIDAD 3', transform=ax2.transAxes,\n",
    "        ha='right', va='bottom', fontweight='bold', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='#3498db', alpha=0.5))\n",
    "ax2.text(0.05, 0.05, 'PRIORIDAD 4', transform=ax2.transAxes,\n",
    "        ha='left', va='bottom', fontweight='bold', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='#95a5a6', alpha=0.5))\n",
    "\n",
    "# GrÃ¡fico 3: Potencial por departamento\n",
    "ax3 = axes[1, 0]\n",
    "dept_potencial = df_potencial.groupby('DEPARTAMENTO')['POTENCIAL_TON'].sum().nlargest(15)\n",
    "bars = ax3.bar(range(len(dept_potencial)), dept_potencial.values/1000,\n",
    "              color=plt.cm.Reds(np.linspace(0.4, 0.9, len(dept_potencial))),\n",
    "              alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "ax3.set_xticks(range(len(dept_potencial)))\n",
    "ax3.set_xticklabels(dept_potencial.index, rotation=45, ha='right', fontsize=9)\n",
    "ax3.set_ylabel('Miles de Toneladas')\n",
    "ax3.set_title('ðŸ—ºï¸ POTENCIAL DE MEJORA POR DEPARTAMENTO', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# GrÃ¡fico 4: Escenarios de mejora\n",
    "ax4 = axes[1, 1]\n",
    "scenarios = {\n",
    "    'Actual': df_combined['VAL_TOTAL'].sum()/1000,\n",
    "    'Si todos al 5%': df_combined['QRESIDUOS_MUN'].sum() * 0.05/1000,\n",
    "    'Si todos al 10%': df_combined['QRESIDUOS_MUN'].sum() * 0.10/1000,\n",
    "    'Si todos al 20%': df_combined['QRESIDUOS_MUN'].sum() * 0.20/1000,\n",
    "    'Meta PLANAA 2024': df_combined['QRESIDUOS_MUN'].sum() * 0.30/1000\n",
    "}\n",
    "colors_sc = ['#e74c3c', '#f39c12', '#3498db', '#27ae60', '#9b59b6']\n",
    "bars = ax4.bar(range(len(scenarios)), list(scenarios.values()), color=colors_sc, alpha=0.8)\n",
    "ax4.set_xticks(range(len(scenarios)))\n",
    "ax4.set_xticklabels(list(scenarios.keys()), rotation=45, ha='right')\n",
    "ax4.set_ylabel('Miles de Toneladas')\n",
    "ax4.set_title('ðŸ“ˆ ESCENARIOS DE MEJORA NACIONAL', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# AÃ±adir valores\n",
    "for bar, val in zip(bars, scenarios.values()):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
    "            f'{val:.0f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('ðŸš€ ANÃLISIS DE OPORTUNIDADES Y POTENCIAL DE MEJORA', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [markdown]\n",
    "# # ðŸŽ¯ INSIGHT 5: Recomendaciones EstratÃ©gicas para Ganar\n",
    "\n",
    "# Calcular KPIs clave\n",
    "total_generado_2023 = df_combined['QRESIDUOS_MUN'].sum()\n",
    "total_valorizado_2023 = df_combined['VAL_TOTAL'].sum()\n",
    "tasa_nacional = (total_valorizado_2023 / total_generado_2023 * 100)\n",
    "potencial_economico = (total_generado_2023 * 0.2 - total_valorizado_2023) * 50  # $50/ton estimado\n",
    "\n",
    "print(\"\\nðŸ“Š KPIs CLAVE 2023:\")\n",
    "print(f\"â€¢ Residuos generados: {total_generado_2023:,.0f} toneladas\")\n",
    "print(f\"â€¢ Residuos valorizados: {total_valorizado_2023:,.0f} toneladas\")\n",
    "print(f\"â€¢ Tasa de valorizaciÃ³n nacional: {tasa_nacional:.2f}%\")\n",
    "print(f\"â€¢ Brecha para alcanzar 20%: {(total_generado_2023*0.2 - total_valorizado_2023):,.0f} toneladas\")\n",
    "print(f\"â€¢ Potencial econÃ³mico sin explotar: ${potencial_economico:,.0f}\")\n",
    "\n",
    "#%%\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preparar datos para modelo predictivo\n",
    "df_model = df_combined.dropna(subset=['TASA_VAL']).copy()\n",
    "\n",
    "# Feature engineering\n",
    "df_model['LOG_POB'] = np.log1p(df_model['POB_TOTAL'])\n",
    "df_model['LOG_GENERADO'] = np.log1p(df_model['QRESIDUOS_MUN'])\n",
    "df_model['DENSIDAD_RESIDUOS'] = df_model['QRESIDUOS_MUN'] / (df_model['POB_TOTAL'] + 1)\n",
    "df_model['ES_CAPITAL'] = (df_model['DISTRITO'] == df_model['PROVINCIA']).astype(int)\n",
    "\n",
    "# Encoding de regiÃ³n natural\n",
    "region_dummies = pd.get_dummies(df_model['REG_NAT'], prefix='REGION')\n",
    "df_model = pd.concat([df_model, region_dummies], axis=1)\n",
    "\n",
    "# Variables para el modelo\n",
    "features = ['LOG_POB', 'LOG_GENERADO', 'DENSIDAD_RESIDUOS', 'ES_CAPITAL'] + list(region_dummies.columns)\n",
    "X = df_model[features]\n",
    "y = df_model['TASA_VAL']\n",
    "\n",
    "# Split y entrenamiento\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# MÃ©tricas\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ¤– MODELO PREDICTIVO DE MACHINE LEARNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“Š Performance del Modelo:\")\n",
    "print(f\"â€¢ RÂ² Score: {r2:.3f}\")\n",
    "print(f\"â€¢ Error Absoluto Medio: {mae:.2f}%\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ”‘ Variables mÃ¡s importantes para predecir eficiencia:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   {i+1}. {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "# VisualizaciÃ³n de predicciones vs reales\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# GrÃ¡fico 1: Predicciones vs Reales\n",
    "ax1.scatter(y_test, y_pred, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax1.set_xlabel('Tasa Real (%)')\n",
    "ax1.set_ylabel('Tasa Predicha (%)')\n",
    "ax1.set_title('ðŸŽ¯ PREDICCIONES DEL MODELO', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.text(0.05, 0.95, f'RÂ² = {r2:.3f}\\nMAE = {mae:.2f}%', \n",
    "         transform=ax1.transAxes, fontsize=12, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# GrÃ¡fico 2: Importancia de variables\n",
    "ax2.barh(range(len(feature_importance.head(10))), \n",
    "         feature_importance.head(10)['importance'].values,\n",
    "         color=plt.cm.viridis(np.linspace(0.3, 0.9, 10)))\n",
    "ax2.set_yticks(range(len(feature_importance.head(10))))\n",
    "ax2.set_yticklabels(feature_importance.head(10)['feature'].values)\n",
    "ax2.set_xlabel('Importancia')\n",
    "ax2.set_title('ðŸ”‘ IMPORTANCIA DE VARIABLES', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('ðŸ¤– MODELO PREDICTIVO PARA OPTIMIZACIÃ“N DE VALORIZACIÃ“N', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#%% [markdown]\n",
    "# # ðŸ“‹ Conclusiones Finales y PrÃ³ximos Pasos\n",
    "\n",
    "#%%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ CONCLUSIONES Y ROADMAP PARA IMPLEMENTACIÃ“N\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nâœ… CONCLUSIONES CLAVE:\")\n",
    "print(\"1. La valorizaciÃ³n en PerÃº estÃ¡ en etapa inicial con ENORME potencial\")\n",
    "print(\"2. Existe una correlaciÃ³n inversa entre tamaÃ±o poblacional y eficiencia\")\n",
    "print(\"3. Los residuos orgÃ¡nicos muestran mayor dinamismo de crecimiento\")\n",
    "print(\"4. La tecnologÃ­a y los incentivos son claves para escalar\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ ROADMAP DE IMPLEMENTACIÃ“N (6 MESES):\")\n",
    "print(\"\\nFASE 1 - PILOTO (Mes 1-2):\")\n",
    "print(\"â€¢ Seleccionar 5 distritos piloto (mix de tamaÃ±os)\")\n",
    "print(\"â€¢ Implementar sistema de tracking bÃ¡sico\")\n",
    "print(\"â€¢ Establecer baseline de mediciÃ³n\")\n",
    "\n",
    "print(\"\\nFASE 2 - EXPANSIÃ“N (Mes 3-4):\")\n",
    "print(\"â€¢ Escalar a 20 distritos\")\n",
    "print(\"â€¢ Lanzar app mÃ³vil ciudadana\")\n",
    "print(\"â€¢ Implementar sistema de incentivos\")\n",
    "\n",
    "print(\"\\nFASE 3 - CONSOLIDACIÃ“N (Mes 5-6):\")\n",
    "print(\"â€¢ Alcanzar 50 distritos objetivo\")\n",
    "print(\"â€¢ Integrar IoT y analytics avanzado\")\n",
    "print(\"â€¢ Medir impacto y ajustar modelo\")\n",
    "\n",
    "print(\"\\nðŸ’° MODELO DE NEGOCIO:\")\n",
    "print(\"â€¢ Ingresos por venta de material valorizado: $30-50/ton\")\n",
    "print(\"â€¢ Bonos de carbono: $15-25/ton CO2eq evitada\")\n",
    "print(\"â€¢ Subsidios gubernamentales y cooperaciÃ³n internacional\")\n",
    "print(\"â€¢ Fee por gestiÃ³n municipal: $5-10/ton procesada\")\n",
    "\n",
    "print(\"\\nðŸ† VENTAJA COMPETITIVA:\")\n",
    "print(\"â€¢ Primera soluciÃ³n integral data-driven en PerÃº\")\n",
    "print(\"â€¢ Modelo replicable en toda LatinoamÃ©rica\")\n",
    "print(\"â€¢ Alineado con ODS y NDCs del paÃ­s\")\n",
    "print(\"â€¢ Genera valor econÃ³mico, social y ambiental\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELDA 1: INSTALACIÃ“N DE DEPENDENCIAS ADICIONALES\n",
    "# =============================================================================\n",
    "# pip install scikit-learn seaborn plotly-dash streamlit folium streamlit-folium\n",
    "\n",
    "# =============================================================================\n",
    "# CELDA 2: IMPORTS ADICIONALES Y CONFIGURACIÃ“N\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_absolute_error\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELDA 0: CARGA, LIMPIEZA Y PREPARACIÃ“N DE DATOS\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def find_waste_file(keyword):\n",
    "    \"\"\"Busca archivo en ubicaciones comunes usando un keyword ('Org' o 'Inorg').\"\"\"\n",
    "    possible_paths = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd() / \"data\",\n",
    "        Path.cwd().parent / \"data\",\n",
    "    ]\n",
    "    for path in possible_paths:\n",
    "        if path.exists():\n",
    "            for f in path.iterdir():\n",
    "                if keyword in f.name:\n",
    "                    return f\n",
    "    raise FileNotFoundError(f\"No se encontrÃ³ archivo con '{keyword}' en {possible_paths}\")\n",
    "\n",
    "def load_and_clean_data(filepath, value_col_name):\n",
    "    \"\"\"\n",
    "    Carga y limpia un dataset de residuos sÃ³lidos.\n",
    "    - Lee el CSV con separador ';' y codificaciÃ³n 'latin1'.\n",
    "    - Limpia los nombres de las columnas (quita espacios).\n",
    "    - Convierte columnas numÃ©ricas a tipos de datos correctos, eliminando comas.\n",
    "    \"\"\"\n",
    "    # Cargar datos especificando separador y codificaciÃ³n\n",
    "    df = pd.read_csv(filepath, sep=';', encoding='latin1', thousands=',')\n",
    "    \n",
    "    # Limpiar nombres de columnas (eliminar espacios extra)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Definir columnas que deben ser numÃ©ricas\n",
    "    numeric_cols = [\n",
    "        'POB_TOTAL', 'POB_URBANA', 'POB_RURAL', 'QRESIDUOS_MUN', value_col_name\n",
    "    ]\n",
    "    \n",
    "    # Convertir columnas a tipo numÃ©rico, manejando errores\n",
    "    for col in numeric_cols:\n",
    "        # La opciÃ³n thousands=',' en read_csv ya deberÃ­a haber manejado esto.\n",
    "        # Si aÃºn hay problemas, se puede forzar con el siguiente cÃ³digo:\n",
    "        # df[col] = df[col].astype(str).str.replace(',', '').astype(float)\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Renombrar columnas para que sean mÃ¡s manejables\n",
    "    df = df.rename(columns={\n",
    "        'QRESIDUOS_MUN': 'GENERADO_TON',\n",
    "        value_col_name: 'VALORIZADO_TON'\n",
    "    })\n",
    "\n",
    "    print(f\"Archivo cargado y limpiado: {filepath.name}\")\n",
    "    print(f\"Columnas detectadas: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "# Cargar y limpiar ambos datasets\n",
    "inorg_df = load_and_clean_data(\n",
    "    find_waste_file(\"Inorg\"),\n",
    "    'QRESIDUOS _VAL_INORGAN'\n",
    ")\n",
    "org_df = load_and_clean_data(\n",
    "    find_waste_file(\"Org\"),\n",
    "    'QRESIDUOS _VAL_ORGAN'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff69c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELDA 1: INSTALACIÃ“N DE DEPENDENCIAS ADICIONALES\n",
    "# =============================================================================\n",
    "# Descomentar si no se han instalado\n",
    "# %pip install scikit-learn seaborn plotly-dash streamlit folium streamlit-folium\n",
    "\n",
    "# =============================================================================\n",
    "# CELDA 2: IMPORTS ADICIONALES Y CONFIGURACIÃ“N\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ConfiguraciÃ³n de estilo\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELDA 3: FUNCIONES DE PROCESAMIENTO DE DATOS MEJORADAS\n",
    "# =============================================================================\n",
    "def fix_encoding_issues(df):\n",
    "    \"\"\"Corrige problemas de encoding en columnas de texto (opcional si se usa 'latin1' al cargar)\"\"\"\n",
    "    df = df.copy()\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in text_cols:\n",
    "        df[col] = (df[col].astype(str)\n",
    "                   .str.replace('ÃƒÂ³', 'Ã³').str.replace('ÃƒÂ¡', 'Ã¡').str.replace('ÃƒÂ©', 'Ã©')\n",
    "                   .str.replace('ÃƒÂ­', 'Ã­').str.replace('ÃƒÂº', 'Ãº').str.replace('ÃƒÂ±', 'Ã±'))\n",
    "    return df\n",
    "\n",
    "def create_master_dataset(org_df, inorg_df):\n",
    "    \"\"\"Combina datasets orgÃ¡nico e inorgÃ¡nico en uno maestro.\"\"\"\n",
    "    # Renombrar columnas de valorizado para la fusiÃ³n\n",
    "    org_clean = org_df.rename(columns={'VALORIZADO_TON': 'ORG_TON'})\n",
    "    inorg_clean = inorg_df.rename(columns={'VALORIZADO_TON': 'INORG_TON'})\n",
    "    \n",
    "    # Columnas clave para agrupar y fusionar\n",
    "    key_cols = ['UBIGEO', 'DEPARTAMENTO', 'PROVINCIA', 'DISTRITO', 'PERIODO']\n",
    "    \n",
    "    # Agrupar por distrito y periodo para evitar duplicados\n",
    "    org_agg = org_clean.groupby(key_cols).agg({\n",
    "        'ORG_TON': 'sum',\n",
    "        'POB_TOTAL': 'first',\n",
    "        'GENERADO_TON': 'first' # La basura generada es la misma\n",
    "    }).reset_index()\n",
    "    \n",
    "    inorg_agg = inorg_clean.groupby(key_cols).agg({\n",
    "        'INORG_TON': 'sum',\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Combinar datasets\n",
    "    master = pd.merge(org_agg, inorg_agg, on=key_cols, how='outer').fillna(0)\n",
    "    \n",
    "    # Crear variables derivadas\n",
    "    master['TOTAL_VALORIZADO'] = master['ORG_TON'] + master['INORG_TON']\n",
    "    master['GENERADO_TOTAL'] = master['GENERADO_TON'] # Usar la columna ya consolidada\n",
    "    master['PORC_VALORIZADO'] = np.where(master['GENERADO_TOTAL'] > 0, \n",
    "                                       (master['TOTAL_VALORIZADO'] / master['GENERADO_TOTAL']) * 100, 0)\n",
    "    master['VALORIZADO_PER_CAPITA'] = np.where(master['POB_TOTAL'] > 0,\n",
    "                                             master['TOTAL_VALORIZADO'] / master['POB_TOTAL'], 0)\n",
    "    \n",
    "    # Crear categorÃ­as de rendimiento\n",
    "    master['CATEGORIA_RENDIMIENTO'] = pd.cut(master['PORC_VALORIZADO'], \n",
    "                                           bins=[-np.inf, 5, 15, 30, np.inf], \n",
    "                                           labels=['Bajo', 'Medio', 'Alto', 'Excelente'])\n",
    "    \n",
    "    return master[['UBIGEO', 'DEPARTAMENTO', 'PROVINCIA', 'DISTRITO', 'PERIODO', 'POB_TOTAL',\n",
    "                   'ORG_TON', 'INORG_TON', 'TOTAL_VALORIZADO', 'GENERADO_TOTAL',\n",
    "                   'PORC_VALORIZADO', 'VALORIZADO_PER_CAPITA', 'CATEGORIA_RENDIMIENTO']]\n",
    "\n",
    "# Crear dataset maestro\n",
    "# Nota: El anÃ¡lisis original sumaba todos los aÃ±os. Para un anÃ¡lisis mÃ¡s granular,\n",
    "# se podrÃ­a agrupar por distrito y mantener los datos por 'PERIODO'.\n",
    "# Por ahora, agregamos todo para seguir la estructura original.\n",
    "master_df_by_year = create_master_dataset(org_df, inorg_df)\n",
    "master_df = master_df_by_year.groupby(['UBIGEO', 'DEPARTAMENTO', 'PROVINCIA', 'DISTRITO']).agg({\n",
    "    'POB_TOTAL': 'last', # Tomar la poblaciÃ³n mÃ¡s reciente\n",
    "    'ORG_TON': 'sum',\n",
    "    'INORG_TON': 'sum',\n",
    "    'TOTAL_VALORIZADO': 'sum',\n",
    "    'GENERADO_TOTAL': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Recalcular mÃ©tricas en el dataframe agregado\n",
    "master_df['PORC_VALORIZADO'] = np.where(master_df['GENERADO_TOTAL'] > 0, (master_df['TOTAL_VALORIZADO'] / master_df['GENERADO_TOTAL']) * 100, 0)\n",
    "master_df['VALORIZADO_PER_CAPITA'] = np.where(master_df['POB_TOTAL'] > 0, master_df['TOTAL_VALORIZADO'] / master_df['POB_TOTAL'], 0)\n",
    "master_df['CATEGORIA_RENDIMIENTO'] = pd.cut(master_df['PORC_VALORIZADO'], bins=[-np.inf, 5, 15, 30, np.inf], labels=['Bajo', 'Medio', 'Alto', 'Excelente'])\n",
    "\n",
    "\n",
    "print(f\"Dataset master creado: {master_df.shape}\")\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cadb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELDA 4: ANÃLISIS EXPLORATORIO AVANZADO\n",
    "# =============================================================================\n",
    "def create_comprehensive_eda(df):\n",
    "    \"\"\"Crea anÃ¡lisis exploratorio comprensivo\"\"\"\n",
    "    \n",
    "    # 1. DistribuciÃ³n de valorizaciÃ³n por regiÃ³n natural\n",
    "    fig1 = px.box(df, x='DEPARTAMENTO', y='PORC_VALORIZADO', \n",
    "                  title='DistribuciÃ³n % ValorizaciÃ³n por Departamento',\n",
    "                  color='DEPARTAMENTO')\n",
    "    fig1.update_xaxes(tickangle=45)\n",
    "    fig1.show()\n",
    "    \n",
    "    # 2. CorrelaciÃ³n entre variables\n",
    "    corr_cols = ['POB_TOTAL', 'TOTAL_VALORIZADO', 'PORC_VALORIZADO', 'VALORIZADO_PER_CAPITA']\n",
    "    corr_matrix = df[corr_cols].corr()\n",
    "    \n",
    "    fig2 = px.imshow(corr_matrix, text_auto=True, aspect=\"auto\",\n",
    "                     title=\"Matriz de CorrelaciÃ³n - Variables Clave\")\n",
    "    fig2.show()\n",
    "    \n",
    "    # 3. AnÃ¡lisis de outliers\n",
    "    fig3 = make_subplots(rows=2, cols=2, \n",
    "                        subplot_titles=['Total Valorizado', 'PoblaciÃ³n', \n",
    "                                      '% Valorizado', 'Per CÃ¡pita'])\n",
    "    \n",
    "    for i, col in enumerate(['TOTAL_VALORIZADO', 'POB_TOTAL', 'PORC_VALORIZADO', 'VALORIZADO_PER_CAPITA']):\n",
    "        row = i // 2 + 1\n",
    "        col_pos = i % 2 + 1\n",
    "        fig3.add_trace(go.Box(y=df[col], name=col), row=row, col=col_pos)\n",
    "    \n",
    "    fig3.update_layout(title_text=\"AnÃ¡lisis de Outliers\", showlegend=False)\n",
    "    fig3.show()\n",
    "    \n",
    "    # 4. Top y Bottom performers\n",
    "    top_10 = df.nlargest(10, 'PORC_VALORIZADO')[['DISTRITO', 'DEPARTAMENTO', 'PORC_VALORIZADO']]\n",
    "    bottom_10 = df.nsmallest(10, 'PORC_VALORIZADO')[['DISTRITO', 'DEPARTAMENTO', 'PORC_VALORIZADO']]\n",
    "    \n",
    "    print(\"ðŸ† TOP 10 - Mayor % ValorizaciÃ³n:\")\n",
    "    print(top_10.to_string(index=False))\n",
    "    print(\"\\nâš ï¸  BOTTOM 10 - Menor % ValorizaciÃ³n:\")\n",
    "    print(bottom_10.to_string(index=False))\n",
    "\n",
    "# Ejecutar EDA\n",
    "create_comprehensive_eda(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6216185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELDA 5: MACHINE LEARNING - CLUSTERING DE DISTRITOS\n",
    "# =============================================================================\n",
    "def perform_clustering(df):\n",
    "    \"\"\"Realiza clustering de distritos por patrones de valorizaciÃ³n\"\"\"\n",
    "    \n",
    "    # Preparar datos para clustering\n",
    "    features = ['POB_TOTAL', 'TOTAL_VALORIZADO', 'PORC_VALORIZADO', 'VALORIZADO_PER_CAPITA']\n",
    "    X = df[features].fillna(0)\n",
    "    \n",
    "    # Normalizar datos\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Encontrar nÃºmero Ã³ptimo de clusters\n",
    "    inertias = []\n",
    "    K_range = range(2, 11)\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # GrÃ¡fico del codo\n",
    "    fig_elbow = go.Figure()\n",
    "    fig_elbow.add_trace(go.Scatter(x=list(K_range), y=inertias, mode='lines+markers'))\n",
    "    fig_elbow.update_layout(title='MÃ©todo del Codo para Clustering', \n",
    "                           xaxis_title='NÃºmero de Clusters', \n",
    "                           yaxis_title='Inercia')\n",
    "    fig_elbow.show()\n",
    "    \n",
    "    # Aplicar clustering con k=5\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "    df['CLUSTER'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Visualizar clusters\n",
    "    fig_cluster = px.scatter_3d(df, x='POB_TOTAL', y='TOTAL_VALORIZADO', z='PORC_VALORIZADO',\n",
    "                               color='CLUSTER', hover_data=['DISTRITO', 'DEPARTAMENTO'],\n",
    "                               title='Clusters de Distritos por PatrÃ³n de ValorizaciÃ³n')\n",
    "    fig_cluster.show()\n",
    "    \n",
    "    # AnÃ¡lisis de clusters\n",
    "    cluster_summary = df.groupby('CLUSTER')[features].mean().round(2)\n",
    "    print(\"ðŸ“Š Resumen por Cluster:\")\n",
    "    print(cluster_summary)\n",
    "    \n",
    "    return df, kmeans, scaler\n",
    "\n",
    "# Ejecutar clustering\n",
    "master_df, kmeans_model, scaler_model = perform_clustering(master_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
